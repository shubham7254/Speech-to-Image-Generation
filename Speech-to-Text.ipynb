{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# displaying speedch to text using whisper "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: speechrecognition in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (3.11.0)\n",
            "Requirement already satisfied: whisper in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (1.1.10)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (2.5.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from speechrecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from speechrecognition) (4.12.2)\n",
            "Requirement already satisfied: six in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from whisper) (1.16.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from requests>=2.26.0->speechrecognition) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from requests>=2.26.0->speechrecognition) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from requests>=2.26.0->speechrecognition) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from requests>=2.26.0->speechrecognition) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/unet/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install speechrecognition whisper torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import speech_recognition as sr\n",
        "import whisper\n",
        "import torch\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from queue import Queue\n",
        "from tempfile import NamedTemporaryFile\n",
        "from sys import platform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def speech_to_text(whisper_model=\"tiny\", duration=10, energy_threshold=1000, record_timeout=2, phrase_timeout=3):\n",
        "    data_queue = Queue()\n",
        "    recorder = sr.Recognizer()\n",
        "    recorder.energy_threshold = energy_threshold\n",
        "    recorder.dynamic_energy_threshold = False\n",
        "\n",
        "    source = sr.Microphone(sample_rate=16000)\n",
        "\n",
        "    model = f\"{whisper_model}.en\" if whisper_model != \"tiny\" else whisper_model\n",
        "    audio_model = whisper.load_model(model)\n",
        "\n",
        "    temp_file = NamedTemporaryFile().name\n",
        "    transcription = []\n",
        "\n",
        "    with source:\n",
        "        recorder.adjust_for_ambient_noise(source)\n",
        "\n",
        "    def record_callback(_, audio: sr.AudioData) -> None:\n",
        "        data_queue.put(audio.get_raw_data())\n",
        "\n",
        "    recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
        "\n",
        "    print(\"Model loaded.\\n\")\n",
        "    print(\"Start recording or say something that you would like to record...\\n\")\n",
        "    os.system(\"clear\")\n",
        "\n",
        "    end_time = time.time() + duration\n",
        "    last_sample = bytes()\n",
        "    phrase_time = None\n",
        "\n",
        "    while time.time() < end_time:\n",
        "        try:\n",
        "            if data_queue.empty():\n",
        "                time.sleep(0.1)\n",
        "                continue\n",
        "\n",
        "            now = datetime.utcnow()\n",
        "            phrase_complete = phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout)\n",
        "\n",
        "            if phrase_complete:\n",
        "                last_sample = bytes()\n",
        "\n",
        "            phrase_time = now\n",
        "\n",
        "            while not data_queue.empty():\n",
        "                last_sample += data_queue.get()\n",
        "\n",
        "            audio_data = sr.AudioData(last_sample, source.SAMPLE_RATE, source.SAMPLE_WIDTH)\n",
        "            wav_data = io.BytesIO(audio_data.get_wav_data())\n",
        "\n",
        "            with open(temp_file, 'wb') as f:\n",
        "                f.write(wav_data.getvalue())\n",
        "\n",
        "            result = audio_model.transcribe(temp_file, fp16=torch.cuda.is_available())\n",
        "            text = result['text'].strip()\n",
        "\n",
        "            if phrase_complete or not transcription:\n",
        "                transcription.append(text)\n",
        "            else:\n",
        "                transcription[-1] = text\n",
        "\n",
        "            os.system('cls' if os.name == 'nt' else 'clear')\n",
        "            print(\"\\n\".join(transcription))\n",
        "            print('', end='', flush=True)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "\n",
        "    return transcription\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting transcription process...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/unet/lib/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded.\n",
            "\n",
            "Start recording or say something that you would like to record...\n",
            "\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2JDog running on moon\n",
            "\n",
            "Final Transcription:\n",
            "Dog running on moon\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to start recording\n",
        "print(\"Starting transcription process...\\n\")\n",
        "transcription = speech_to_text(whisper_model=\"medium\", duration=10)\n",
        "\n",
        "# Display the final transcription after 10 seconds\n",
        "print(\"\\nFinal Transcription:\")\n",
        "print(\"\\n\".join(transcription))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
